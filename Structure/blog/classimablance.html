<!DOCTYPE html>
<html lang="en">
 <head>
  <title>
   Blog
  </title>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <link href="../CSS_layout.css" media="screen" rel="stylesheet" type="text/css"/>
  <style>
  </style>
 </head>
 <body>
  <div class="navbar">
   <a class="left" float="left" href="../../index.html">
    üåø Home
   </a>
   <a href="../blog.html">
    Blog
   </a>
  </div>
  <div class="blog">
   <h2>
    Deal with class imbalance using Python
   </h2>
   <p align="justify">
    In a dataset including observations belonging to class 0 or class 1,
      if only 5% of observations belong to class 0, then even a naive model that predicts every
      observation as belonging to class 1 will be 95% accurate. Clearly, this is not ideal.
      Using confusion matrices, precision, recall, F1 scores, and ROC curves is always helpful.
      In some cases, though, you actually can deal with this class imbalance.
   </p>
   <p align="justify">
    There are 4 types of solution:
   </p>
   <p align="justify">
    1. The first and the best way would be to collect more data.
   </p>
   <p align="justify">
    2. You could change the metrics used to evaluate your model so that the class imbalance is not an issue anymore.
   </p>
   <p align="justify">
    3. You can use a model‚Äôs built-in class weight parameters if possible; For example:
    <code style="background-color:black;color:yellow;">
     randomforest(class_weight=‚Äùbalanced‚Äù)
    </code>
    <br/>
    <p align="justify">
     4. Finally, if none of the above is feasible, consider downsampling, or upsampling.
    </p>
    <p align="justify">
     As a rule of thumb, downsampling is done when the number of observations in the majority
      class is higher than a third of the number of observations in the minority class. However, it is usually
      considered as better to use upsampling since less information is lost.
    </p>
    <p align="justify">
     We will now see how to downsample and upsample using Python.
    </p>
    <p>
     <b>
      Downsampling the majority class
     </b>
    </p>
    <p align="justify">
     Downsampling means creating a new set of observations from the majority class
      of size equals to the minority class's size. Thus, we randomly sample without replacement
      (we don't want the same observation twice since we have enough observations in this class)
      from the majority class until we reach the size of the minority class. We first extract the observations'
      indexes for each different class (here 0 for minority class, 1 for the majority class):
    </p>
    <code style="background-color:black;color:yellow;">
     # Indicies of each class' observations
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     ind_0 = np.where(target == 0)[0]
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     ind_1 = np.where(target == 1)[0]
    </code>
    <br/>
    <p align="justify">
     We then get the lenght of each class:
    </p>
    <code style="background-color:black;color:yellow;">
     # Number of observations in each class
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     n_0 = len(ind_0)
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     n_1 = len(ind_1)
    </code>
    <br/>
    <p align="justify">
     Finally, we will randomly pick as many observations in the majority
      class as there are observations in the minority class:
    </p>
    <code style="background-color:black;color:yellow;">
     # For every observation of class 0, randomly sample from majority class without replacement
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     ind_1_downsampled = np.random.choice(ind_1, size=n_0, replace=False)
    </code>
    <br/>
    <p align="justify">
     We can now use the indexes that were randomly picked to form the new set
      of majority class observations and build the preprocess dataset with as many observations
      from the majority class as there are in the minority class. We join the downsampled set of
      majority class observations to the set of minority class observations:
    </p>
    <code style="background-color:black;color:yellow;">
     # Join together the vectors of target
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     np.hstack((target[ind_0], target[ind_1_downsampled]))
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     # Join together the arrays of features
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     np.vstack((features[ind_0,:], features[ind_1_downsampled,:]))
    </code>
    <br/>
    <p align="justify">
     That's it, you can now run your analysis. Don't forget to report this
      operation in details in the methods section.
    </p>
    <p>
     <b>
      Upsampling the minority class
     </b>
    </p>
    <p align="justify">
     Downsampling means creating a new set of observations from the minority
      class of size equals to the majority class's size. Thus, we randomly sample with replacement
      from the minority class (we create duplicates on purpose to reach the size of the majority
      class set) until we reach the size of the majority class. So this time, we randomly pick
      as many observations in the minority class as there are observations in the majority class
      (thus indeed creating duplicates). We keep our example thus we have 0 for minority class,
      1 for the majority class. Let's downsample:
    </p>
    <code style="background-color:black;color:yellow;">
     # For every observation in class 1, randomly sample from class 0 with replacement
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     ind_0_upsampled = np.random.choice(ind_0, size=n_1, replace=True)
    </code>
    <br/>
    <p align="justify">
     We can now use the indexes that were randomly picked to form the new
      set of minority class observations and build the preprocess dataset with as many observations
      from the minority class as there are in the majority class. We join the upsampled set of
      minority class observations to the set of majority class observations:
    </p>
    <code style="background-color:black;color:yellow;">
     # Join together the vectors of target
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     np.concatenate((target[ind_0_upsampled], target[ind_1]))
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     # Join together the arrays of features
    </code>
    <br/>
    <code style="background-color:black;color:yellow;">
     np.vstack((features[ind_0_upsampled,:], features[ind_1,:]))
    </code>
    <br/>
    <p align="justify">
     That's it, you can now run your analysis. Don't forget
      to report this operation in details in the methods section.
    </p>
    <p align="justify">
     Source: Chris Albon (2018). Machine Learning with Python Cookbook.
    </p>
   </p>
  </div>
 </body>
</html>
